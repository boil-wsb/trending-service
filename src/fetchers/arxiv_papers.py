"""
arXivè®ºæ–‡æ•°æ®è·å–å™¨
è·å–arXivæœ€æ–°è®ºæ–‡ä¿¡æ¯
"""

import sys
import io

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8ï¼ˆä»…åœ¨äº¤äº’å¼ç¯å¢ƒä¸­ï¼‰
if hasattr(sys.stdout, 'buffer') and not sys.stdout.closed:
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    except:
        pass

import requests
import re
from typing import List, Dict
from pathlib import Path
from datetime import datetime
from urllib.parse import quote

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.config import DATA_SOURCES, REQUESTS
from src.utils import get_logger, save_json
from .base import BaseFetcher, TrendingItem


class ArxivPapersFetcher(BaseFetcher):
    """arXivè®ºæ–‡æ•°æ®è·å–å™¨"""
    
    name = "arxiv"

    def __init__(self, config: Dict = None, logger=None):
        super().__init__(config, logger)
        self.base_url = "http://export.arxiv.org/api/query"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': REQUESTS['user_agent'],
            'Accept': 'application/xml',
        })
        self.logger = logger or get_logger('arxiv_papers')
        self.config = config or DATA_SOURCES['arxiv']

    def fetch(self) -> List[TrendingItem]:
        """
        è·å–arXivè®ºæ–‡ï¼ˆå®ç°åŸºç±»æ–¹æ³•ï¼‰
        
        Returns:
            List[TrendingItem]: çƒ­ç‚¹æ•°æ®åˆ—è¡¨
        """
        limit = self.config.get('limit', 20)
        categories = self.config.get('categories', ['cs.AI', 'cs.LG'])
        
        self.logger.info(f"è·å–arXivè®ºæ–‡ (categories={categories}, limit={limit})...")

        # æ„å»ºæŸ¥è¯¢
        category_query = ' OR '.join([f'cat:{cat}' for cat in categories])
        params = {
            'search_query': category_query,
            'start': 0,
            'max_results': limit,
            'sortBy': 'lastUpdatedDate',
            'sortOrder': 'descending'
        }

        try:
            response = self.session.get(self.base_url, params=params, timeout=REQUESTS['timeout'])
            response.raise_for_status()

            items = self._parse_response(response.text)
            self.logger.info(f"arXiv: è·å– {len(items)} æ¡æ•°æ®")
            return items

        except Exception as e:
            self.logger.error(f"è·å–arXivè®ºæ–‡å¤±è´¥: {e}")
            return []

    def _parse_response(self, xml_text: str) -> List[TrendingItem]:
        """è§£æarXiv APIå“åº”"""
        items = []

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è®ºæ–‡ä¿¡æ¯
        entries = re.findall(r'<entry>(.*?)</entry>', xml_text, re.DOTALL)

        for entry in entries:
            try:
                # æå–ID
                id_match = re.search(r'<id>(.*?)</id>', entry)
                paper_id = id_match.group(1).split('/')[-1] if id_match else ''

                # æå–æ ‡é¢˜
                title_match = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
                title = title_match.group(1).strip().replace('\n', ' ') if title_match else ''

                # æå–æ‘˜è¦
                summary_match = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
                summary = summary_match.group(1).strip().replace('\n', ' ') if summary_match else ''

                # æå–ä½œè€…
                authors = []
                author_matches = re.findall(r'<name>(.*?)</name>', entry)
                for author in author_matches:
                    authors.append(author.strip())

                # æå–å‘å¸ƒæ—¥æœŸ
                published_match = re.search(r'<published>(.*?)</published>', entry)
                published = published_match.group(1) if published_match else ''

                # æå–æ›´æ–°æ—¥æœŸ
                updated_match = re.search(r'<updated>(.*?)</updated>', entry)
                updated = updated_match.group(1) if updated_match else ''

                # æå–åˆ†ç±»
                category_match = re.search(r'<category term="(.*?)"', entry)
                category = category_match.group(1) if category_match else ''

                # æ„å»ºURL
                url = f"https://arxiv.org/abs/{paper_id}"

                item = TrendingItem(
                    source=self.name,
                    title=title,
                    url=url,
                    author=', '.join(authors[:3]) if authors else None,  # åªå–å‰3ä¸ªä½œè€…
                    description=summary,  # å®Œæ•´æè¿°
                    hot_score=None,  # arXiv æ²¡æœ‰çƒ­åº¦åˆ†æ•°
                    category=category,
                    extra={
                        'paper_id': paper_id,
                        'authors': authors,
                        'published': published,
                        'updated': updated,
                        'category': category
                    }
                )
                items.append(item)

            except Exception as e:
                self.logger.warning(f"è§£æè®ºæ–‡å¤±è´¥: {e}")
                continue

        return items

    def fetch_papers(self, categories: List[str] = None, limit: int = None) -> List[Dict]:
        """
        è·å–arXivè®ºæ–‡ï¼ˆæ—§æ¥å£ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰

        Args:
            categories: è®ºæ–‡åˆ†ç±»åˆ—è¡¨
            limit: è¿”å›è®ºæ–‡æ•°é‡é™åˆ¶

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        items = self.fetch()
        return [
            {
                'id': item.extra.get('paper_id', ''),
                'title': item.title,
                'url': item.url,
                'summary': item.description,
                'authors': item.extra.get('authors', []),
                'published': item.extra.get('published', ''),
                'updated': item.extra.get('updated', ''),
                'category': item.category
            }
            for item in items
        ]

    def save_json(self, papers: List[Dict], filepath: Path) -> None:
        """ä¿å­˜è®ºæ–‡æ•°æ®åˆ°JSONæ–‡ä»¶"""
        data = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "papers": []
        }

        for idx, paper in enumerate(papers, 1):
            data["papers"].append({
                "rank": idx,
                "id": paper.get('id', ''),
                "title": paper.get('title', ''),
                "url": paper.get('url', '#'),
                "summary": paper.get('summary', ''),
                "authors": paper.get('authors', []),
                "published": paper.get('published', ''),
                "updated": paper.get('updated', ''),
                "category": paper.get('category', '')
            })

        save_json(data, filepath)
        self.logger.info(f"æ•°æ®å·²ä¿å­˜: {filepath}")

    def fetch_all(self, output_dir: Path) -> Dict[str, Path]:
        """è·å–æ‰€æœ‰arXivæ•°æ®å¹¶ä¿å­˜"""
        self.logger.info("å¼€å§‹è·å–arXivè®ºæ–‡æ•°æ®...")

        result = {}
        
        # å®šä¹‰åˆ†ç±»æ˜ å°„
        categories = {
            'biology': ['q-bio', 'q-bio.CB', 'q-bio.GN', 'q-bio.MN', 'q-bio.NC', 'q-bio.OT', 'q-bio.PE', 'q-bio.QM', 'q-bio.SC', 'q-bio.TO'],
            'computer_ai': ['cs.AI', 'cs.LG', 'cs.CV', 'cs.NE', 'cs.RO']
        }
        
        # è·å–ç”Ÿç‰©åˆ†ç±»è®ºæ–‡
        biology_papers = self.fetch_papers(categories=categories['biology'])
        if biology_papers:
            filepath = output_dir / 'arxiv_biology.json'
            self.save_json(biology_papers, filepath)
            result['arxiv_biology'] = filepath
        else:
            self.logger.error("è·å–ç”Ÿç‰©åˆ†ç±»è®ºæ–‡å¤±è´¥")
        
        # è·å–è®¡ç®—æœº-äººå·¥æ™ºèƒ½åˆ†ç±»è®ºæ–‡
        computer_ai_papers = self.fetch_papers(categories=categories['computer_ai'])
        if computer_ai_papers:
            filepath = output_dir / 'arxiv_computer_ai.json'
            self.save_json(computer_ai_papers, filepath)
            result['arxiv_computer_ai'] = filepath
        else:
            self.logger.error("è·å–è®¡ç®—æœº-äººå·¥æ™ºèƒ½åˆ†ç±»è®ºæ–‡å¤±è´¥")

        return result


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è·å–arXivè®ºæ–‡æ•°æ®...")
    print(f"â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    from ..config import REPORTS_DIR
    fetcher = ArxivPapersFetcher()

    result = fetcher.fetch_all(REPORTS_DIR)

    print("ğŸ‰ arXivæ•°æ®è·å–å®Œæˆ!")
    return result


if __name__ == "__main__":
    main()
