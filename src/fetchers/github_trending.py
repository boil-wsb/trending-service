"""
GitHubçƒ­é—¨æ•°æ®è·å–å™¨
ç›´æ¥çˆ¬å– GitHub Trending é¡µé¢è·å–çœŸæ­£çš„æœ¬å‘¨çƒ­é—¨é¡¹ç›®
"""

import sys
import io
import requests
import json
import os
import re
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any
from bs4 import BeautifulSoup
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.config import DATA_SOURCES, SOURCE_URLS, REQUESTS, PROXY
from src.utils import get_logger, save_json


class GitHubTrendingFetcher:
    """GitHubçƒ­é—¨æ•°æ®è·å–å™¨"""

    def __init__(self, logger=None):
        self.base_url = "https://github.com"
        self.trending_url = SOURCE_URLS['github_trending']
        self.session = requests.Session()
        
        # æ›´çœŸå®çš„æµè§ˆå™¨è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
        })
        self.logger = logger or get_logger('github_trending')
        self.config = DATA_SOURCES['github']
        self.max_retries = 3
        self.retry_delay = 5

    def fetch_trending_page(self, language: str = "", since: str = "weekly") -> BeautifulSoup:
        """
        çˆ¬å– GitHub Trending é¡µé¢
        language: ç¼–ç¨‹è¯­è¨€ï¼Œå¦‚ "python", "javascript", "" è¡¨ç¤ºæ‰€æœ‰è¯­è¨€
        since: æ—¶é—´å‘¨æœŸï¼Œ"daily", "weekly", "monthly"
        """
        url = self.trending_url
        params = {}

        if language:
            url = f"{self.trending_url}/{language}"

        if since:
            params['since'] = since

        # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
        delay = random.uniform(2, 5)
        self.logger.info(f"ç­‰å¾… {delay:.1f} ç§’åè¯·æ±‚...")
        time.sleep(delay)

        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                self.logger.info(f"è¯·æ±‚: {url}?since={since} (å°è¯• {attempt + 1}/{self.max_retries})")
                
                # è®¾ç½®ä»£ç†
                proxies = None
                if PROXY.get('enabled'):
                    proxies = {
                        'http': PROXY.get('http', ''),
                        'https': PROXY.get('https', '')
                    }
                
                response = self.session.get(
                    url, 
                    params=params, 
                    timeout=REQUESTS['timeout'],
                    proxies=proxies
                )
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                return soup
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    wait_time = self.retry_delay * (attempt + 1) + random.uniform(1, 3)
                    self.logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    self.logger.error(f"è·å–Trendingé¡µé¢å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡: {e}")
                    return None
            except Exception as e:
                self.logger.error(f"è·å–Trendingé¡µé¢å¤±è´¥: {e}")
                return None

    def parse_repos(self, soup: BeautifulSoup, limit: int = 20) -> List[Dict]:
        """è§£æ Trending é¡µé¢ä¸­çš„ä»“åº“ä¿¡æ¯"""
        repos = []

        # æŸ¥æ‰¾æ‰€æœ‰ä»“åº“æ¡ç›®
        repo_articles = soup.find_all('article', class_='Box-row')

        for article in repo_articles[:limit]:
            try:
                # è·å–ä»“åº“åç§°å’Œé“¾æ¥
                title_element = article.find('h2', class_='h3 lh-condensed')
                if not title_element:
                    continue

                link_element = title_element.find('a')
                if not link_element:
                    continue

                full_name = link_element.get('href', '').lstrip('/')
                repo_url = f"{self.base_url}{link_element.get('href', '')}"

                # è·å–æè¿°
                desc_element = article.find('p', class_='col-9')
                description = desc_element.get_text(strip=True) if desc_element else ''

                # è·å–ç¼–ç¨‹è¯­è¨€
                language_element = article.find('span', itemprop='programmingLanguage')
                language = language_element.get_text(strip=True) if language_element else 'Unknown'

                # è·å– stars æ•°é‡
                stars_element = article.find('a', href=lambda x: x and '/stargazers' in x)
                stars = 0
                if stars_element:
                    stars_text = stars_element.get_text(strip=True)
                    stars = self.parse_number(stars_text)

                # è·å– forks æ•°é‡
                forks_element = article.find('a', href=lambda x: x and '/forks' in x)
                forks = 0
                if forks_element:
                    forks_text = forks_element.get_text(strip=True)
                    forks = self.parse_number(forks_text)

                # è·å–æœ¬å‘¨æ–°å¢ stars
                current_stars_element = article.find('span', class_='d-inline-block float-sm-right')
                current_period_stars = 0
                if current_stars_element:
                    stars_text = current_stars_element.get_text(strip=True)
                    match = re.search(r'(\d+(?:,\d+)*)\s+stars', stars_text)
                    if match:
                        current_period_stars = int(match.group(1).replace(',', ''))

                # è·å–ä½œè€…/è´¡çŒ®è€…å¤´åƒ
                built_by = []
                avatars = article.find_all('img', class_='avatar mb-1')
                for avatar in avatars:
                    built_by.append({
                        'username': avatar.get('alt', ''),
                        'avatar': avatar.get('src', '')
                    })

                repos.append({
                    'full_name': full_name,
                    'url': repo_url,
                    'description': description,
                    'language': language,
                    'stars': stars,
                    'forks': forks,
                    'currentPeriodStars': current_period_stars,
                    'builtBy': built_by,
                    'updatedAt': ''
                })

            except Exception as e:
                self.logger.warning(f"è§£æä»“åº“å¤±è´¥: {e}")
                continue

        return repos

    def parse_number(self, text: str) -> int:
        """è§£ææ•°å­—å­—ç¬¦ä¸²ï¼Œå¦‚ '1.2k' -> 1200"""
        text = text.strip().lower()

        if 'k' in text:
            num = float(text.replace('k', '').replace(',', ''))
            return int(num * 1000)

        return int(text.replace(',', ''))

    def get_trending_repos(self, per_page: int = None) -> List[Dict]:
        """è·å–æœ¬å‘¨çƒ­é—¨ä»“åº“ï¼ˆæ‰€æœ‰è¯­è¨€ï¼‰"""
        per_page = per_page or self.config['limit']
        self.logger.info("è·å–æœ¬å‘¨çƒ­é—¨ä»“åº“...")

        soup = self.fetch_trending_page(language="", since=self.config['since'])
        if not soup:
            return []

        repos = self.parse_repos(soup, limit=per_page)
        self.logger.info(f"è·å–åˆ° {len(repos)} ä¸ªçƒ­é—¨ä»“åº“")
        return repos

    def get_ai_repos(self, per_page: int = None) -> List[Dict]:
        """è·å–AIé¢†åŸŸçƒ­é—¨é¡¹ç›®"""
        per_page = per_page or self.config['limit']
        self.logger.info("è·å–AIé¢†åŸŸçƒ­é—¨é¡¹ç›®...")

        # è·å– Python çƒ­é—¨é¡¹ç›®ï¼ˆAIé¡¹ç›®å¤šä¸ºPythonï¼‰
        soup = self.fetch_trending_page(language="python", since=self.config['since'])
        if not soup:
            return []

        python_repos = self.parse_repos(soup, limit=50)

        # AIå…³é”®è¯
        ai_keywords = [
            'ai', 'artificial', 'intelligence', 'machine', 'learning', 'ml',
            'deep', 'neural', 'network', 'tensorflow', 'pytorch', 'keras',
            'llm', 'gpt', 'chatgpt', 'claude', 'transformer', 'nlp',
            'computer', 'vision', 'cv', 'reinforcement', 'diffusion',
            'stable', 'diffusion', 'openai', 'anthropic', 'hugging',
            'langchain', 'agent', 'rag', 'embedding', 'vector', 'model',
            'inference', 'training', 'fine-tune', 'finetune', 'whisper',
            'segment', 'controlnet', 'midjourney', 'dalle', 'stable-diffusion',
            'autogpt', 'babyagi', 'chat', 'bot', 'copilot', 'assistant'
        ]

        ai_repos = []
        for repo in python_repos:
            name = repo.get('full_name', '').lower()
            description = repo.get('description', '').lower()

            is_ai = any(keyword in name or keyword in description for keyword in ai_keywords)
            if is_ai:
                ai_repos.append(repo)

        self.logger.info(f"ä» {len(python_repos)} ä¸ªPythoné¡¹ç›®ä¸­ç­›é€‰å‡º {len(ai_repos)} ä¸ªAIé¡¹ç›®")

        # å¦‚æœAIé¡¹ç›®ä¸å¤Ÿï¼Œè¡¥å……æ›´å¤šPythoné¡¹ç›®
        if len(ai_repos) < per_page:
            additional = per_page - len(ai_repos)
            for repo in python_repos:
                if repo not in ai_repos and len(ai_repos) < per_page:
                    ai_repos.append(repo)
                if len(ai_repos) >= per_page:
                    break

        return ai_repos[:per_page]

    def save_json(self, repos: List[Dict], filename: str, repo_type: str = "repos") -> None:
        """ä¿å­˜JSONæ•°æ®"""
        data = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            repo_type: []
        }

        for idx, repo in enumerate(repos, 1):
            data[repo_type].append({
                "rank": idx,
                "name": repo.get('full_name', 'N/A'),
                "url": repo.get('url', '#'),
                "description": repo.get('description') or '',
                "stars": repo.get('stars', 0),
                "forks": repo.get('forks', 0),
                "language": repo.get('language', 'Unknown'),
                "current_period_stars": repo.get('currentPeriodStars', 0),
                "built_by": repo.get('builtBy', []),
                "updated_at": repo.get('updatedAt', '')
            })

        filepath = Path(filename)
        save_json(data, filepath)
        self.logger.info(f"æ•°æ®å·²ä¿å­˜: {filepath}")

    def fetch_all(self, output_dir: Path) -> Dict[str, Path]:
        """è·å–æ‰€æœ‰GitHubæ•°æ®å¹¶ä¿å­˜"""
        self.logger.info("å¼€å§‹è·å–GitHubçƒ­é—¨æ•°æ®...")

        result = {}

        # è·å–æœ¬å‘¨çƒ­é—¨ä»“åº“
        trending_repos = self.get_trending_repos(per_page=50)
        if trending_repos:
            # æŒ‰æœ¬å‘¨æ–°å¢ stars æ’åº
            sorted_by_growth = sorted(trending_repos, key=lambda x: x.get('currentPeriodStars', 0), reverse=True)
            top_growth = sorted_by_growth[:self.config['limit']]

            # æŒ‰æ€»starsæ’åº
            sorted_by_total = sorted(trending_repos, key=lambda x: x.get('stars', 0), reverse=True)
            top_total = sorted_by_total[:self.config['limit']]

            # ä¿å­˜æ•°æ®
            growth_file = output_dir / 'github_weekly_growth.json'
            trending_file = output_dir / 'github_trending.json'

            self.save_json(top_growth, growth_file)
            self.save_json(top_total, trending_file)

            result['github_weekly_growth'] = growth_file
            result['github_trending'] = trending_file
        else:
            self.logger.error("è·å–çƒ­é—¨ä»“åº“å¤±è´¥")

        # è·å–AIé¡¹ç›®
        ai_repos = self.get_ai_repos(per_page=self.config['limit'])
        if ai_repos:
            ai_file = output_dir / 'ai_trending.json'
            self.save_json(ai_repos, ai_file)
            result['ai_trending'] = ai_file
        else:
            self.logger.error("è·å–AIé¡¹ç›®å¤±è´¥")

        return result


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è·å–GitHubçƒ­é—¨æ•°æ®...")
    print(f"â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    from ..config import REPORTS_DIR
    fetcher = GitHubTrendingFetcher()

    # è·å–æ‰€æœ‰æ•°æ®
    result = fetcher.fetch_all(REPORTS_DIR)

    print("ğŸ‰ GitHubæ•°æ®è·å–å®Œæˆ!")
    return result


if __name__ == "__main__":
    main()