"""
GitHubçƒ­é—¨æ•°æ®è·å–å™¨
ç›´æ¥çˆ¬å– GitHub Trending é¡µé¢è·å–çœŸæ­£çš„æœ¬å‘¨çƒ­é—¨é¡¹ç›®
"""

import sys
import io
import requests
import json
import os
import re
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any
from bs4 import BeautifulSoup
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.config import DATA_SOURCES, SOURCE_URLS, REQUESTS, PROXY
from src.utils import get_logger, save_json
from .base import BaseFetcher, TrendingItem


class GitHubTrendingFetcher(BaseFetcher):
    """GitHubçƒ­é—¨æ•°æ®è·å–å™¨"""
    
    name = "github"

    def __init__(self, config: Dict = None, logger=None):
        super().__init__(config, logger)
        self.base_url = "https://github.com"
        self.trending_url = SOURCE_URLS['github_trending']
        self.session = requests.Session()
        
        # æ›´çœŸå®çš„æµè§ˆå™¨è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
        })
        self.logger = logger or get_logger('github_trending')
        self.config = config or DATA_SOURCES['github']
        self.max_retries = 3
        self.retry_delay = 5

    def fetch_trending_page(self, language: str = "", since: str = "weekly") -> BeautifulSoup:
        """
        çˆ¬å– GitHub Trending é¡µé¢
        language: ç¼–ç¨‹è¯­è¨€ï¼Œå¦‚ "python", "javascript", "" è¡¨ç¤ºæ‰€æœ‰è¯­è¨€
        since: æ—¶é—´å‘¨æœŸï¼Œ"daily", "weekly", "monthly"
        """
        url = self.trending_url
        params = {}

        if language:
            url = f"{self.trending_url}/{language}"

        if since:
            params['since'] = since

        # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
        delay = random.uniform(2, 5)
        self.logger.info(f"ç­‰å¾… {delay:.1f} ç§’åè¯·æ±‚...")
        time.sleep(delay)

        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                self.logger.info(f"è¯·æ±‚: {url}?since={since} (å°è¯• {attempt + 1}/{self.max_retries})")
                
                # è®¾ç½®ä»£ç†
                proxies = None
                if PROXY.get('enabled'):
                    proxies = {
                        'http': PROXY.get('http', ''),
                        'https': PROXY.get('https', '')
                    }
                
                response = self.session.get(
                    url, 
                    params=params, 
                    timeout=REQUESTS['timeout'],
                    proxies=proxies
                )
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                return soup
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    wait_time = self.retry_delay * (attempt + 1) + random.uniform(1, 3)
                    self.logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    self.logger.error(f"è·å–Trendingé¡µé¢å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡: {e}")
                    return None
            except Exception as e:
                self.logger.error(f"è·å–Trendingé¡µé¢å¤±è´¥: {e}")
                return None

    def parse_repos(self, soup: BeautifulSoup, limit: int = 20) -> List[Dict]:
        """è§£æ Trending é¡µé¢ä¸­çš„ä»“åº“ä¿¡æ¯"""
        repos = []

        # æŸ¥æ‰¾æ‰€æœ‰ä»“åº“æ¡ç›®
        repo_articles = soup.find_all('article', class_='Box-row')

        for article in repo_articles[:limit]:
            try:
                # è·å–ä»“åº“åç§°å’Œé“¾æ¥
                title_element = article.find('h2', class_='h3 lh-condensed')
                if not title_element:
                    continue

                link_element = title_element.find('a')
                if not link_element:
                    continue

                full_name = link_element.get('href', '').lstrip('/')
                repo_url = f"{self.base_url}{link_element.get('href', '')}"

                # è·å–æè¿°
                desc_element = article.find('p', class_='col-9')
                description = desc_element.get_text(strip=True) if desc_element else ''

                # è·å–ç¼–ç¨‹è¯­è¨€
                language_element = article.find('span', itemprop='programmingLanguage')
                language = language_element.get_text(strip=True) if language_element else 'Unknown'

                # è·å– stars æ•°é‡
                stars_element = article.find('a', href=lambda x: x and '/stargazers' in x)
                stars = 0
                if stars_element:
                    stars_text = stars_element.get_text(strip=True)
                    stars = self.parse_number(stars_text)

                # è·å– forks æ•°é‡
                forks_element = article.find('a', href=lambda x: x and '/forks' in x)
                forks = 0
                if forks_element:
                    forks_text = forks_element.get_text(strip=True)
                    forks = self.parse_number(forks_text)

                # è·å–æœ¬å‘¨æ–°å¢ stars
                current_stars_element = article.find('span', class_='d-inline-block float-sm-right')
                current_period_stars = 0
                if current_stars_element:
                    stars_text = current_stars_element.get_text(strip=True)
                    match = re.search(r'(\d+(?:,\d+)*)\s+stars', stars_text)
                    if match:
                        current_period_stars = int(match.group(1).replace(',', ''))

                # è·å–ä½œè€…/è´¡çŒ®è€…å¤´åƒ
                built_by = []
                avatars = article.find_all('img', class_='avatar mb-1')
                for avatar in avatars:
                    built_by.append({
                        'username': avatar.get('alt', ''),
                        'avatar': avatar.get('src', '')
                    })

                repos.append({
                    'full_name': full_name,
                    'url': repo_url,
                    'description': description,
                    'language': language,
                    'stars': stars,
                    'forks': forks,
                    'currentPeriodStars': current_period_stars,
                    'builtBy': built_by,
                    'updatedAt': ''
                })

            except Exception as e:
                self.logger.warning(f"è§£æä»“åº“å¤±è´¥: {e}")
                continue

        return repos

    def parse_number(self, text: str) -> int:
        """è§£ææ•°å­—å­—ç¬¦ä¸²ï¼Œå¦‚ '1.2k' -> 1200"""
        text = text.strip().lower()

        if 'k' in text:
            num = float(text.replace('k', '').replace(',', ''))
            return int(num * 1000)

        return int(text.replace(',', ''))

    def fetch(self) -> List[TrendingItem]:
        """
        è·å–GitHubçƒ­é—¨æ•°æ®ï¼ˆå®ç°åŸºç±»æ–¹æ³•ï¼‰
        
        Returns:
            List[TrendingItem]: çƒ­ç‚¹æ•°æ®åˆ—è¡¨
        """
        self.logger.info("å¼€å§‹è·å–GitHubçƒ­é—¨æ•°æ®...")
        
        items = []
        
        # è·å–æœ¬å‘¨çƒ­é—¨ä»“åº“
        soup = self.fetch_trending_page(language="", since=self.config.get('since', 'weekly'))
        if soup:
            repos = self.parse_repos(soup, limit=self.config.get('limit', 20))
            for repo in repos:
                item = TrendingItem(
                    source=self.name,
                    title=repo.get('full_name', ''),
                    url=repo.get('url', ''),
                    author=repo.get('builtBy')[0].get('username') if repo.get('builtBy') else None,
                    description=repo.get('description'),
                    hot_score=float(repo.get('currentPeriodStars', 0)),
                    category=repo.get('language'),
                    extra={
                        'stars': repo.get('stars', 0),
                        'forks': repo.get('forks', 0),
                        'language': repo.get('language', 'Unknown'),
                        'built_by': repo.get('builtBy', [])
                    }
                )
                items.append(item)
        
        self.logger.info(f"GitHub: è·å– {len(items)} æ¡æ•°æ®")
        return items

    def get_ai_repos(self) -> List[TrendingItem]:
        """è·å–AIé¢†åŸŸçƒ­é—¨é¡¹ç›®"""
        self.logger.info("è·å–AIé¢†åŸŸçƒ­é—¨é¡¹ç›®...")

        # è·å– Python çƒ­é—¨é¡¹ç›®ï¼ˆAIé¡¹ç›®å¤šä¸ºPythonï¼‰
        soup = self.fetch_trending_page(language="python", since=self.config.get('since', 'weekly'))
        if not soup:
            return []

        repos = self.parse_repos(soup, limit=50)

        # AIå…³é”®è¯
        ai_keywords = [
            'ai', 'artificial', 'intelligence', 'machine', 'learning', 'ml',
            'deep', 'neural', 'network', 'tensorflow', 'pytorch', 'keras',
            'llm', 'gpt', 'chatgpt', 'claude', 'transformer', 'nlp',
            'computer', 'vision', 'cv', 'reinforcement', 'diffusion',
            'stable', 'diffusion', 'openai', 'anthropic', 'hugging',
            'langchain', 'agent', 'rag', 'embedding', 'vector', 'model',
            'inference', 'training', 'fine-tune', 'finetune', 'whisper',
            'segment', 'controlnet', 'midjourney', 'dalle', 'stable-diffusion',
            'autogpt', 'babyagi', 'chat', 'bot', 'copilot', 'assistant'
        ]

        ai_items = []
        for repo in repos:
            name = repo.get('full_name', '').lower()
            description = repo.get('description', '').lower()

            is_ai = any(keyword in name or keyword in description for keyword in ai_keywords)
            if is_ai:
                item = TrendingItem(
                    source=f"{self.name}_ai",
                    title=repo.get('full_name', ''),
                    url=repo.get('url', ''),
                    author=repo.get('builtBy')[0].get('username') if repo.get('builtBy') else None,
                    description=repo.get('description'),
                    hot_score=float(repo.get('currentPeriodStars', 0)),
                    category='AI',
                    extra={
                        'stars': repo.get('stars', 0),
                        'forks': repo.get('forks', 0),
                        'language': repo.get('language', 'Unknown'),
                        'built_by': repo.get('builtBy', [])
                    }
                )
                ai_items.append(item)

        self.logger.info(f"ä» {len(repos)} ä¸ªPythoné¡¹ç›®ä¸­ç­›é€‰å‡º {len(ai_items)} ä¸ªAIé¡¹ç›®")

        # å¦‚æœAIé¡¹ç›®ä¸å¤Ÿï¼Œè¡¥å……æ›´å¤šPythoné¡¹ç›®
        if len(ai_items) < self.config.get('limit', 20):
            additional = self.config.get('limit', 20) - len(ai_items)
            for repo in repos:
                if len(ai_items) >= self.config.get('limit', 20):
                    break
                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨åˆ—è¡¨ä¸­
                repo_name = repo.get('full_name', '')
                if not any(item.title == repo_name for item in ai_items):
                    item = TrendingItem(
                        source=f"{self.name}_ai",
                        title=repo.get('full_name', ''),
                        url=repo.get('url', ''),
                        author=repo.get('builtBy')[0].get('username') if repo.get('builtBy') else None,
                        description=repo.get('description'),
                        hot_score=float(repo.get('currentPeriodStars', 0)),
                        category=repo.get('language'),
                        extra={
                            'stars': repo.get('stars', 0),
                            'forks': repo.get('forks', 0),
                            'language': repo.get('language', 'Unknown'),
                            'built_by': repo.get('builtBy', [])
                        }
                    )
                    ai_items.append(item)

        return ai_items[:self.config.get('limit', 20)]

    def fetch_all(self) -> List[TrendingItem]:
        """è·å–æ‰€æœ‰GitHubæ•°æ®"""
        self.logger.info("å¼€å§‹è·å–GitHubçƒ­é—¨æ•°æ®...")

        all_items = []

        # è·å–æœ¬å‘¨çƒ­é—¨ä»“åº“
        trending_items = self.fetch()
        all_items.extend(trending_items)

        # è·å–AIé¡¹ç›®
        ai_items = self.get_ai_repos()
        all_items.extend(ai_items)

        self.logger.info(f"GitHub: æ€»å…±è·å– {len(all_items)} æ¡æ•°æ®")
        return all_items


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è·å–GitHubçƒ­é—¨æ•°æ®...")
    print(f"â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    fetcher = GitHubTrendingFetcher()

    # è·å–æ‰€æœ‰æ•°æ®
    items = fetcher.fetch_all()

    print(f"ğŸ‰ GitHubæ•°æ®è·å–å®Œæˆ! å…± {len(items)} æ¡")
    
    # æ˜¾ç¤ºå‰5æ¡
    for i, item in enumerate(items[:5], 1):
        print(f"{i}. {item.title} (çƒ­åº¦: {item.hot_score})")
    
    return items


if __name__ == "__main__":
    main()
